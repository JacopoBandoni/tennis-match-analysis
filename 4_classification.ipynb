{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Metrics and model evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "\n",
    "# Classifiers\n",
    "import wittgenstein as lw\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## Decision tree visualization\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import display\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pio.templates.default = \"seaborn\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "- STANDARDIZZARE \n",
    "  - MODELLI (neural networks)\n",
    "  - METHOD EVALUATION WITH OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "def get_grid_search(estimator, parameters):\n",
    "  return GridSearchCV(estimator, param_grid=parameters, cv=5, n_jobs=-1, refit=True, return_train_score=True)\n",
    "\n",
    "# pretty printing of metrics computed on test set\n",
    "def report_scores(y_test, y_pred):\n",
    "  print(classification_report(y_test, y_pred, target_names=classes, zero_division=0, sample_weight=None))\n",
    "  # train_pred_dt = dt.predict(x_train)\n",
    "  # y_pred = dt.predict(x_test)\n",
    "  # print('Accuracy training set ', metrics.accuracy_score(y_train, train_pred_dt))\n",
    "  # print('Accuracy test set ', metrics.accuracy_score(y_test, y_pred))\n",
    "  # print('Precision training set ', metrics.precision_score(y_train, train_pred_dt, average='weighted'))\n",
    "  # print('Precision test set ', metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "  # print('Recall training set ', metrics.recall_score(y_train, train_pred_dt, average='weighted'))\n",
    "  # print('F1 score trainig set ', metrics.f1_score(y_train, train_pred_dt, average='weighted'))\n",
    "  # print('Support training set ', metrics.precision_recall_fscore_support(y_train, train_pred_dt))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False):\n",
    "  if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "    print('Confusion matrix, without normalization')\n",
    "\n",
    "  # px.imshow(cm, x=classes, y=classes, title='Confusion matrix', color_continuous_scale=\"Blues\", labels=dict(x=\"Real value\", y=\"Predicted value\", color=\"Records\"), text_auto=True)\n",
    "  fig = ff.create_annotated_heatmap(cm[[1, 0]], x=classes, y=classes, colorscale='Blues', showscale=True)\n",
    "  fig.update_layout(xaxis = dict(title='Predicted value'), yaxis = dict(title='Real value'))\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def plot_learning_curve(clf, X, y, scorer='accuracy', cv=5, train_sizes=np.linspace(.1, 1.0, 5), \n",
    "                        shuffle=False, random_state=None):\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, train_sizes=train_sizes, cv=cv,\n",
    "                                                            scoring=scorer, n_jobs=-1, shuffle=shuffle,\n",
    "                                                            random_state=random_state)\n",
    "\n",
    "    mean_train_score = np.mean(train_scores, axis=1)\n",
    "    std_train_score = np.std(train_scores, axis=1)\n",
    "    mean_test_score = np.mean(test_scores, axis=1)\n",
    "    std_test_score = np.std(test_scores, axis=1)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score, name='train score', line=dict(color='royalblue')))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score + std_train_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score - std_train_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(65,105,225,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score, name='cross-validation score', line=dict(color='firebrick')))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score + std_test_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score - std_test_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(255,107,107,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    model_name = str(clf.__class__.__name__)\n",
    "    fig.update_layout(title=f'Learning Curve for {model_name}',\n",
    "                   xaxis_title='Train set size',\n",
    "                   yaxis_title='Accuracy')\n",
    "    fig.show()\n",
    "\n",
    "def print_grid_search_results(clf):\n",
    "  df = pd.DataFrame(clf.cv_results_)[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score')\n",
    "  display(df.head(10).style.set_caption('Top 10 Grid Search results').hide_index())\n",
    "\n",
    "def report(clf, X, Y, y_test, y_pred, model_classes):\n",
    "  print_grid_search_results(clf)\n",
    "  cm = confusion_matrix(y_test, y_pred, labels=model_classes)\n",
    "  plot_confusion_matrix(cm, classes=classes)\n",
    "  report_scores(y_test, y_pred)\n",
    "  plot_learning_curve(clf.best_estimator_, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete = pd.read_csv(\"./datasets/players.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per il momento, gli stessi attributi utilizzati per il clustering REM\n",
    "# problema: la feature mean_rank_points, da utilizzare come label, era gi√† presente tra le features utilizzate per il clustering\n",
    "df_players = df_players_complete[['max_tourney_revenue', 'lrpOnMxrp', 'matches_won_ratio', 'mean_rank_points']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['low_ranked', 'high_ranked']\n",
    "df_players['is_high_ranked'] = np.digitize(df_players['mean_rank_points'], bins=[df_players['mean_rank_points'].median()])\n",
    "df_players[['mean_rank_points', 'is_high_ranked']]\n",
    "df_players.drop(columns=['mean_rank_points'], inplace=True)\n",
    "df_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.is_high_ranked.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_norm = pd.DataFrame(MinMaxScaler().fit_transform(df_players), columns=df_players.columns)\n",
    "\n",
    "X = df_players_norm.drop(columns=['is_high_ranked'])\n",
    "Y = df_players_norm['is_high_ranked']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4], 'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}\n",
    "clf = get_grid_search(DecisionTreeClassifier(), parameters).fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "models.append((\"Decision Tree\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdot_data = tree.export_graphviz(clf.best_estimator_, out_file=None,\n",
    "                         feature_names=list(x_train.columns),\n",
    "                         class_names=classes,\n",
    "                         filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(cdot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "#TODO: problema, cos'√® il rettangolo nero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule based (RIPPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"prune_size\": [0.33, 0.5], \"k\": [1, 2]}\n",
    "clf = get_grid_search(lw.RIPPER(), parameters).fit(x_train, y_train, pos_class=1)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "models.append((\"Rule based\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.out_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4], 'n_estimators': [10, 20, 50, 100]}\n",
    "clf = get_grid_search(RandomForestClassifier(), parameters).fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "models.append((\"Random Forest\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors': [2,3,4,5,6,7,8,9,10], 'weights':['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute'], 'metric': ['euclidean', 'manhattan', 'minkowski']}\n",
    "clf = get_grid_search(KNeighborsClassifier(), parameters).fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test.values)\n",
    "\n",
    "models.append((\"KNN\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test.values, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "clf = get_grid_search(GaussianNB(), parameters).fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "models.append((\"Naive Bayes\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "clf = get_grid_search(SVC(probability=True), parameters).fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "models.append((\"SVM\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "parameters = {\"solver\": ['lbfgs', 'sgd', 'adam'], \"alpha\": [0.0001, 0.001, 0.01], \"hidden_layer_sizes\": [(10,), (20,)], \"activation\": ['identity', 'logistic', 'tanh', 'relu'], \"learning_rate\": ['constant', 'invscaling', 'adaptive']}\n",
    "clf = get_grid_search(MLPClassifier(), parameters).fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "models.append((\"Neural Network\", clf.best_estimator_))\n",
    "report(clf, X, Y, y_test, y_pred, clf.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=1, y1=1, line=dict(color=\"RoyalBlue\",width=3, dash=\"dash\"))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    y_score = models[i][1].predict_proba(x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    auc_score = roc_auc_score(y_test, y_score)\n",
    "    models[i] += (auc_score,)\n",
    "    \n",
    "# Sort according to AUC score\n",
    "models.sort(key=lambda x: x[2], reverse=True)  \n",
    "for model in models:\n",
    "    y_score = model[1].predict_proba(x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    name = f\"{model[0]} - AUC={model[2]:.3f}\"\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=700, height=500,\n",
    "    autosize=True\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f625ebbd10e36045cf734aa94df07d176492c240bf54797abcd7e809b7fa9e5b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
