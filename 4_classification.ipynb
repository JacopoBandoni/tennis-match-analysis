{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Metrics and model evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve, validation_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Classifiers\n",
    "import wittgenstein as lw\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "## Decision tree visualization\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from IPython.display import display\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pio.templates.default = \"seaborn\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "def get_grid_search(estimator, parameters, normalization=True):\n",
    "  estimator.random_state = 42\n",
    "  if normalization:\n",
    "      pipeline = Pipeline(steps=[('scaler', MinMaxScaler()), ('clf', estimator)])\n",
    "  else:\n",
    "      pipeline = Pipeline(steps=[('clf', estimator)])\n",
    "\n",
    "  return GridSearchCV(pipeline, param_grid=parameters, cv=5, n_jobs=-1, refit=True, return_train_score=True)\n",
    "\n",
    "def print_grid_search_results(clf):\n",
    "  df = pd.DataFrame(clf.cv_results_)[['params', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score')\n",
    "  display(df.head(10).style.set_caption('Top 10 Grid Search results').hide_index())\n",
    "\n",
    "def report_scores(y_test, y_pred):\n",
    "  print(classification_report(y_test, y_pred, target_names=classes, zero_division=0, sample_weight=None))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False):\n",
    "  if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "    print('Confusion matrix, without normalization')\n",
    "\n",
    "  # px.imshow(cm, x=classes, y=classes, title='Confusion matrix', color_continuous_scale=\"Blues\", labels=dict(x=\"Real value\", y=\"Predicted value\", color=\"Records\"), text_auto=True)\n",
    "  fig = ff.create_annotated_heatmap(cm[[1, 0]], x=classes, y=classes, colorscale='Blues', showscale=True)\n",
    "  fig.update_layout(xaxis = dict(title='Predicted value'), yaxis = dict(title='Real value'))\n",
    "  fig.show()\n",
    "\n",
    "def plot_learning_curve(clf, X, y, scoring='accuracy', cv=5, train_sizes=np.linspace(.1, 1.0, 5), \n",
    "                        shuffle=False, random_state=None):\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(clf, X, y, train_sizes=train_sizes, cv=cv,\n",
    "                                                            scoring=scoring, n_jobs=-1, shuffle=shuffle,\n",
    "                                                            random_state=random_state)\n",
    "    mean_train_score = np.mean(train_scores, axis=1)\n",
    "    std_train_score = np.std(train_scores, axis=1)\n",
    "    mean_test_score = np.mean(test_scores, axis=1)\n",
    "    std_test_score = np.std(test_scores, axis=1)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score, name='train score', line=dict(color='royalblue')))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score + std_train_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_train_score - std_train_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(65,105,225,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score, name='cross-validation score', line=dict(color='firebrick')))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score + std_test_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "    fig.add_trace(go.Scatter(x=train_sizes, y=mean_test_score - std_test_score, fill='tonexty', showlegend=False, \n",
    "      fillcolor='rgba(255,107,107,0.2)',\n",
    "      line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "    model_name = str(clf.__class__.__name__)\n",
    "    fig.update_layout(title=f'Learning Curve for {model_name}',\n",
    "                   xaxis_title='Train set size',\n",
    "                   yaxis_title='Accuracy')\n",
    "    fig.show()\n",
    "\n",
    "def plot_validation_curve(clf, X, y, parameters, validation_parameter, scoring='accuracy', cv=5):\n",
    "  param_range = parameters[validation_parameter]\n",
    "  train_scores, test_scores = validation_curve(clf, X, y, param_name=validation_parameter, param_range=param_range,\n",
    "                                                 cv=cv, scoring=scoring, n_jobs=-1)\n",
    "  mean_train_score = np.mean(train_scores, axis=1)\n",
    "  std_train_score = np.std(train_scores, axis=1)\n",
    "  mean_test_score = np.mean(test_scores, axis=1)\n",
    "  std_test_score = np.std(test_scores, axis=1)\n",
    "\n",
    "  fig = go.Figure()\n",
    "\n",
    "  fig.add_trace(go.Scatter(x=param_range, y=mean_train_score, name='train score', line=dict(color='royalblue')))\n",
    "  fig.add_trace(go.Scatter(x=param_range, y=mean_train_score + std_train_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "  fig.add_trace(go.Scatter(x=param_range, y=mean_train_score - std_train_score, fill='tonexty', showlegend=False, \n",
    "    fillcolor='rgba(65,105,225,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "  fig.add_trace(go.Scatter(x=param_range, y=mean_test_score, name='cross-validation score', line=dict(color='firebrick')))\n",
    "  fig.add_trace(go.Scatter(x=param_range, y=mean_test_score + std_test_score, mode=\"lines\", showlegend=False, line=dict(width=0)))\n",
    "  fig.add_trace(go.Scatter(x=param_range, y=mean_test_score - std_test_score, fill='tonexty', showlegend=False, \n",
    "    fillcolor='rgba(255,107,107,0.2)',\n",
    "    line_color='rgba(255,255,255,0)',))\n",
    "\n",
    "  model_name = str(clf.__class__.__name__)\n",
    "  fig.update_layout(title=f'Validation Curve for {model_name}',\n",
    "                  xaxis_title=validation_parameter,\n",
    "                  yaxis_title='Accuracy')\n",
    "  fig.show()\n",
    "\n",
    "\n",
    "def report(clf, X, Y, y_test, y_pred, model_classes, parameters=None, validation_parameter=None):\n",
    "  print_grid_search_results(clf)\n",
    "  cm = confusion_matrix(y_test, y_pred, labels=model_classes)\n",
    "  plot_confusion_matrix(cm, classes=classes)\n",
    "  report_scores(y_test, y_pred)\n",
    "  if parameters and validation_parameter:\n",
    "    plot_validation_curve(clf.best_estimator_, X, Y, parameters, validation_parameter)\n",
    "  plot_learning_curve(clf.best_estimator_, X, Y)\n",
    "\n",
    "def fit_and_report(name, estimator, parameters, X, Y, x_train, y_train, x_test, y_test, validation_parameter, normalization=True):\n",
    "  # add clf to parameters\n",
    "  parameters = {'clf__' + k: v for k, v in parameters.items()}\n",
    "  validation_parameter = 'clf__' + validation_parameter\n",
    "\n",
    "  clf = get_grid_search(estimator, parameters, normalization).fit(x_train, y_train)\n",
    "  y_pred = clf.predict(x_test)\n",
    "  models.append((name, clf.best_estimator_))\n",
    "  report(clf, X, Y, y_test, y_pred, clf.classes_, parameters, validation_parameter)\n",
    "  return clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete = pd.read_csv(\"./datasets/players.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete['is_high_ranked'] = np.digitize(df_players_complete['mean_rank_points'], bins=[df_players_complete['mean_rank_points'].median()])\n",
    "print(df_players_complete['is_high_ranked'].value_counts())\n",
    "px.histogram(df_players_complete, x=\"mean_rank_points\", color=\"is_high_ranked\", title=\"Histogram of mean rank points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pareto splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest 80% of mean rank points are considered low ranked\n",
    "df_players_complete['is_high_ranked'] = np.digitize(df_players_complete['mean_rank_points'], bins=[df_players_complete['mean_rank_points'].quantile(0.8)])\n",
    "print(df_players_complete['is_high_ranked'].value_counts())\n",
    "px.histogram(df_players_complete, x=\"mean_rank_points\", color=\"is_high_ranked\", title=\"Histogram of mean rank points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply k-means to identify cluster of good and bad players\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "feautures = ['max_tourney_revenue', 'mean_rank_points', 'lrpOnMxrp', 'matches_won_ratio']\n",
    "df_data = df_players_complete[feautures].reset_index(drop=True)\n",
    "#df_data = pd.DataFrame(MinMaxScaler().fit_transform(df_data), columns=df_data.columns)\n",
    "#df_data = df_data.round(3)\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, max_iter=100, init=\"k-means++\", random_state=42).fit(df_data)\n",
    "df_players_complete['cluster'] = kmeans.labels_\n",
    "\n",
    "# is high ranked based on cluster results\n",
    "df_players_complete['is_high_ranked'] = df_players_complete['cluster']\n",
    "# print number of players\n",
    "print(df_players_complete['is_high_ranked'].value_counts())\n",
    "px.histogram(df_players_complete, x=\"mean_rank_points\", color=\"is_high_ranked\", title=\"Histogram of mean rank points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choice of the label to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['low_ranked', 'high_ranked']\n",
    "df_players = df_players_complete.copy()\n",
    "df_players['is_high_ranked'] = np.digitize(df_players['mean_rank_points'], bins=[df_players['mean_rank_points'].quantile(0.8)])\n",
    "df_players[['mean_rank_points', 'is_high_ranked']]\n",
    "df_players.drop(columns=['mean_rank_points'], inplace=True)\n",
    "df_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.is_high_ranked.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per il momento, gli stessi attributi utilizzati per il clustering REM\n",
    "# problema: la feature mean_rank_points, da utilizzare come label, era già presente tra le features utilizzate per il clustering\n",
    "# df_players = df_players_complete[['max_tourney_revenue', 'lrpOnMxrp', 'matches_won_ratio', 'mean_rank_points']]\n",
    "\n",
    "# select all numerical features\n",
    "features_to_drop = ['ht', 'max_rank_points', 'variance_rank_points', 'max_tourney_spectators', 'max_tourney_revenue', 'last_rank_points'] \n",
    "df_players = df_players.dropna(subset=['rel_bpSaved']).drop(features_to_drop,axis=1)\n",
    "df_players = df_players.select_dtypes(include=['int64', 'float64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_players_norm = pd.DataFrame(MinMaxScaler().fit_transform(df_players), columns=df_players.columns)\n",
    "\n",
    "X = df_players.drop(columns=['is_high_ranked'])\n",
    "Y = df_players['is_high_ranked']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4,5,6,7,8,9,10], 'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}\n",
    "best_model = fit_and_report(\"Decision Tree\", DecisionTreeClassifier(), parameters, X, Y, x_train, y_train, x_test, y_test, 'max_depth', normalization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdot_data = tree.export_graphviz(best_model[\"clf\"], out_file=None,\n",
    "                         feature_names=list(x_train.columns),\n",
    "                         class_names=classes,\n",
    "                         filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(cdot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "#TODO: problema, cos'è il rettangolo nero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of the most important features according to the model\n",
    "# map the feature importances to the feature names\n",
    "feature_importances = pd.DataFrame({'feature': X.columns, 'importance': best_model[\"clf\"].feature_importances_})\n",
    "# sort according to the importance\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False).reset_index(drop=True)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule based (RIPPER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\"prune_size\": [0.33, 0.5], \"k\": [1, 2]}\n",
    "# clf = get_grid_search(lw.RIPPER(), parameters).fit(x_train, y_train, pos_class=1)\n",
    "# y_pred = clf.predict(x_test)\n",
    "\n",
    "# models.append((\"Rule based\", clf.best_estimator_))\n",
    "# report(clf, X, Y, y_test, y_pred, clf.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.best_estimator_.out_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth': [2,3,4,5,6,7,8,9,10], 'n_estimators': [10, 20, 50, 100]}\n",
    "best_model = fit_and_report(\"Random Forest\", RandomForestClassifier(), parameters, X, Y, x_train, y_train, x_test, y_test, 'max_depth', normalization=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQRT heuristic on train set to find the optimal K\n",
    "k_euristic = int(np.sqrt(len(x_train))) + 1\n",
    "k_range = list(range(1, k_euristic, 2))\n",
    "\n",
    "parameters = {'n_neighbors': k_range, 'weights':['uniform', 'distance'], 'algorithm': ['ball_tree', 'kd_tree', 'brute'], 'metric': ['euclidean', 'manhattan', 'minkowski']}\n",
    "best_model = fit_and_report(\"KNN\", KNeighborsClassifier(), parameters, X, Y, x_train, y_train, x_test, y_test, 'n_neighbors', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {}\n",
    "best_model = fit_and_report(\"Naive Bayes\", GaussianNB(), parameters, X, Y, x_train, y_train, x_test, y_test, '', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "best_model = fit_and_report(\"SVM\", SVC(probability=True), parameters, X, Y, x_train, y_train, x_test, y_test, 'C', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"solver\": ['lbfgs', 'sgd', 'adam'], \"alpha\": [0.0001, 0.001, 0.01], \"hidden_layer_sizes\": [(10,), (20,)], \"activation\": ['identity', 'logistic', 'tanh', 'relu'], \"learning_rate\": ['constant', 'invscaling', 'adaptive']}\n",
    "best_model = fit_and_report(\"Neural Network\", MLPClassifier(), parameters, X, Y, x_train, y_train, x_test, y_test, 'alpha', normalization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=1, y1=1, line=dict(color=\"RoyalBlue\",width=3, dash=\"dash\"))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    y_score = models[i][1].predict_proba(x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    auc_score = roc_auc_score(y_test, y_score)\n",
    "    models[i] += (auc_score,)\n",
    "    \n",
    "# Sort according to AUC score\n",
    "models.sort(key=lambda x: x[2], reverse=True)  \n",
    "for model in models:\n",
    "    y_score = model[1].predict_proba(x_test)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    name = f\"{model[0]} - AUC={model[2]:.3f}\"\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=700, height=500,\n",
    "    autosize=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for the model with the best AUC score\n",
    "best_model = max(models, key=lambda x: x[2])\n",
    "\n",
    "# PCA\n",
    "X_r = pd.DataFrame(PCA(n_components=2).fit_transform(x_train))\n",
    "prediction = best_model[1].predict_proba(x_train)\n",
    "fig = px.scatter(x=X_r[0], y=X_r[1], color=prediction[:, 1], color_continuous_scale='RdBu', symbol=y_train, symbol_map={'0': 'square-dot', '1': 'circle-dot'},  labels={'symbol': 'label', 'color': 'score of <br>first class'})\n",
    "fig.update_traces(marker_size=12, marker_line_width=1.5)\n",
    "fig.update_layout(title=f\"PCA visualization for {best_model[0]}\", legend_orientation='h')\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f625ebbd10e36045cf734aa94df07d176492c240bf54797abcd7e809b7fa9e5b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
