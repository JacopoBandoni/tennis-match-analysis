{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: riguardare i testi indicati con il tag TEMP<br>\n",
    "TODO: rimuovere i testi con il tag REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Metrics and model evaluation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Classifiers\n",
    "from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "## Decision tree visualization\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pio.templates.default = \"seaborn\"\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions\n",
    "- STANDARDIZZARE \n",
    "  - classification report\n",
    "  - LEARNING CURVE\n",
    "  - MODELLI\n",
    "  - METHOD EVALUATION WITH OVERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pretty printing of metrics computed on test set\n",
    "def report_scores(test_label, test_pred):\n",
    "  print(classification_report(test_label, test_pred, target_names=classes, zero_division=0, sample_weight=None))\n",
    "  # train_pred_dt = dt.predict(train_set)\n",
    "  # test_pred_dt = dt.predict(test_set)\n",
    "  # print('Accuracy training set ', metrics.accuracy_score(train_label, train_pred_dt))\n",
    "  # print('Accuracy test set ', metrics.accuracy_score(test_label, test_pred_dt))\n",
    "  # print('Precision training set ', metrics.precision_score(train_label, train_pred_dt, average='weighted'))\n",
    "  # print('Precision test set ', metrics.precision_score(test_label, test_pred_dt, average='weighted'))\n",
    "  # print('Recall training set ', metrics.recall_score(train_label, train_pred_dt, average='weighted'))\n",
    "  # print('F1 score trainig set ', metrics.f1_score(train_label, train_pred_dt, average='weighted'))\n",
    "  # print('Support training set ', metrics.precision_recall_fscore_support(train_label, train_pred_dt))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False):\n",
    "  if normalize:\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(\"Normalized confusion matrix\")\n",
    "  else:\n",
    "    print('Confusion matrix, without normalization')\n",
    "\n",
    "  # px.imshow(cm, x=classes, y=classes, title='Confusion matrix', color_continuous_scale=\"Blues\", labels=dict(x=\"Real value\", y=\"Predicted value\", color=\"Records\"), text_auto=True)\n",
    "  fig = ff.create_annotated_heatmap(cm[[1, 0]], x=classes, y=classes, colorscale='Blues', showscale=True)\n",
    "  fig.update_layout(xaxis = dict(title='Predicted value'), yaxis = dict(title='Real value'))\n",
    "  fig.show()\n",
    "\n",
    "def report(test_label, test_pred, model_classes):\n",
    "  cm = confusion_matrix(test_label, test_pred, labels=model_classes)\n",
    "  plot_confusion_matrix(cm, classes=classes)\n",
    "  report_scores(test_label, test_pred)\n",
    "  \n",
    "models = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_complete = pd.read_csv(\"./datasets/players.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# per il momento, gli stessi attributi utilizzati per il clustering REM\n",
    "# problema: la feature mean_rank_points, da utilizzare come label, era già presente tra le features utilizzate per il clustering\n",
    "df_players = df_players_complete[['max_tourney_revenue', 'lrpOnMxrp', 'matches_won_ratio', 'mean_rank_points']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['low_ranked', 'high_ranked']\n",
    "df_players['is_high_ranked'] = np.digitize(df_players['mean_rank_points'], bins=[df_players['mean_rank_points'].median()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players[['mean_rank_points', 'is_high_ranked']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.drop(columns=['mean_rank_points'], inplace=True)\n",
    "df_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.is_high_ranked.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players_norm = pd.DataFrame(MinMaxScaler().fit_transform(df_players), columns=df_players.columns)\n",
    "\n",
    "X = df_players_norm.drop(columns=['is_high_ranked'])\n",
    "Y = df_players_norm['is_high_ranked']\n",
    "train_set, test_set, train_label, test_label = train_test_split(X, Y, stratify=Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: REM da questo punto in poi si tratta di copia e incolla adattati (dal progetto di Pasquali ecc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'criterion':['gini','entropy'],'max_depth':[2, 3, 4]}\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, cv=5)\n",
    "clf.fit(train_set, train_label)\n",
    "pd.DataFrame(clf.cv_results_)[['params', 'mean_test_score']].sort_values('mean_test_score', ascending = False)\n",
    "\n",
    "#TODO: REM clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(criterion=clf.best_params_['criterion'], splitter='best', max_depth=clf.best_params_['max_depth'],\n",
    " min_samples_split=3, min_samples_leaf=8)\n",
    "dt.fit(train_set, train_label)\n",
    "test_pred_dt = dt.predict(test_set)\n",
    "\n",
    "models.append((\"Decision Tree\", dt))\n",
    "report(test_label, test_pred_dt, dt.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdot_data = tree.export_graphviz(dt, out_file=None,\n",
    "                         feature_names=list(train_set.columns),\n",
    "                         class_names=classes,\n",
    "                         filled=True, rounded=True)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n",
    "#TODO: problema, cos'è il rettangolo nero?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree', metric='minkowski')\n",
    "knn.fit(train_set, train_label)\n",
    "test_pred_knn = knn.predict(test_set)\n",
    "\n",
    "models.append((\"KNN\", knn))\n",
    "report(test_label, test_pred_knn, knn.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(train_set, train_label)\n",
    "test_pred_gnb = gnb.predict(test_set)\n",
    "models.append((\"Gaussian Naive Bayes\", gnb))\n",
    "\n",
    "report(test_label, test_pred_gnb, gnb.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_shape(type=\"line\", x0=0, y0=0, x1=1, y1=1, line=dict(color=\"RoyalBlue\",width=3, dash=\"dash\"))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    y_score = models[i][1].predict_proba(test_set)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(test_label, y_score)\n",
    "    auc_score = roc_auc_score(test_label, y_score)\n",
    "    models[i] += (auc_score,)\n",
    "    \n",
    "models.sort(key=lambda x: x[2], reverse=True)    \n",
    "for model in models:\n",
    "    name = f\"{model[0]} - AUC={model[2]:.3f}\"\n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, name=name, mode='lines'))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='False Positive Rate',\n",
    "    yaxis_title='True Positive Rate',\n",
    "    yaxis=dict(scaleanchor=\"x\", scaleratio=1),\n",
    "    xaxis=dict(constrain='domain'),\n",
    "    width=700, height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = fig.data[0].name.split(\"AUC=\")[1]\n",
    "\n",
    "# fig.data.sort(key=lambda x: float(x.name.split(\"AUC=\")[1]), reverse=True)\n",
    "# what's the type of auc\n",
    "print(fig.data)\n",
    "\n",
    "for trace in fig.data:\n",
    "  auc = trace.name.split(\"AUC=\")[1]\n",
    "  print(auc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f625ebbd10e36045cf734aa94df07d176492c240bf54797abcd7e809b7fa9e5b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
